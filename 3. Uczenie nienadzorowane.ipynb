{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2168aa9",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4bbed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b09b0c",
   "metadata": {},
   "source": [
    "# 3. Uczenie nienadzorowane\n",
    "\n",
    "W poprzednim zeszycie zbadaliśmy różne modele grafowych sieci neuronowych w scenariuszu nadzorowanej klasyfikacji wierzchołków. Teraz zajmiemy się tematem uczenia nienadzorowanego dla GNNów, który jak już wcześniej wspomnieliśmy, jest trochę bardziej złożony.\n",
    "\n",
    "W jaki sposób powinna być skonstruowana funkcja kosztu? Czy możemy zastosować model autokodera? Jak w takim razie powinien działać dekoder? Jak uwzględnić relacje między wierzchołkami?\n",
    "\n",
    "To tylko kilka pytań, na które należy odpowiedzieć podczas opracowania nienadzorowanego modelu grafowych sieci neuronowych. W ostatnich latach powstało wiele rozwiązań, obejmujących między innymi:\n",
    "- grafowe autokodery (w tym wariacyjne)\n",
    "- uczenie kontrastowe\n",
    "- uczenie samo-nadzorowane\n",
    "\n",
    "W ninejszym zeszycie najpierw zbadamy **model grafowego autokodera** jako najprostszego modelu stosowanego w nienadzorowanym uczeniu reprezentacji grafów, a następnie zbadamy **model Graph Barlow Twins**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ff553",
   "metadata": {},
   "source": [
    "## 3.1. Grafowy autokoder\n",
    "W 2016 roku Kipf, autor pracy wprowadzającej architekturę GCN, opublikował również [artykuł](https://arxiv.org/pdf/1611.07308.pdf) w którym pokazał jak wykorzystać GCNa (lub dowolny inny GNNowy model) w znanej nam architekturze autokodera. Jak wiemy taki model składa się z dwóch komponentów:\n",
    "- **kodera** - w tym wypadku koderem jest wybrana przez nas grafowa sieć neuronowa\n",
    "\n",
    "$$\\mathbf{Z} = \\text{GNN}(\\mathbf{X}, \\mathbf{A})$$\n",
    "\n",
    "- **dekodera** - model dekodera na wejściu przyjmuje wyznaczone reprezentacje $\\mathbf{Z}$, a na wyjściu oblicza rekonstrukcję danego obiektu, w naszym przypadku grafu. Jednak co dokładnie powinien odtworzyć taki grafowy dekoder? Strukturę, atrybuty, czy jedno i drugie? W swojej pracy Kipf zaproponował, aby skupić się wyłącznie na strukturze grafu, tzn. dokonać rekonstrukcji krawędzi. Taki wariant również będziemy rozważać na cele tego laboratorium, przy czym inne scenariusze są równie poprawne i w zależności od konkretnego zadania mogą dostarczać lepszych wyników. \n",
    "\n",
    "W celu zbudowania odpowiedniego dekodera strukturalnego musimy określić w jaki sposób będziemy decydować czy istnieje krawędź między parą dowolnych wierzchołków. Najpopularniejszym rozwiązaniem jest wykorzystanie iloczynu skalarnego, podobnie jak w przypadku modelu Node2vec definiowaliśmy podobieństwo wierzchołków w przestrzeni reprezentacji. Tutaj wybór ten jest umotywowany intuicją, że podobne wierzchołki powinny być połączone krawędzią. Dekoder ma zatem postać:\n",
    "\n",
    "$$\\hat{\\mathbf{A}} = \\sigma(\\mathbf{Z}\\mathbf{Z}^T),$$\n",
    "\n",
    "gdzie:\n",
    "- $\\hat{\\mathbf{A}}$ to rekonstrukcja macierzy sąsiedztwa\n",
    "- $\\sigma(\\cdot)$ to sigmoidalna funkcja aktywacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66095d",
   "metadata": {},
   "source": [
    "## Zadanie 3.1. (4 pkt)\n",
    "Wykorzystując zaimplementowane w PyTorch-Geometricu modele:\n",
    "- grafowego autokodera `GAE` - [link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GAE.html)\n",
    "- dekodera iloczynu skalarnego `InnerProductDecoder` - [link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.InnerProductDecoder.html)\n",
    "\n",
    "dokończ implementację klasy `BaseUnsupervisedGNN` oraz `GraphAutoencoder`. Zastosuj się do komentarzy umieszczonych przy odpowiednich funkcjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba78ae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "236b1e897712b61d0e10db372fad9df1",
     "grade": true,
     "grade_id": "base-unsupervised",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class BaseUnsupervisedGNN(pl.LightningModule):\n",
    "    \"\"\"Base class for unsupervised GNN models for node representations.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._downstream_model = None\n",
    "        self.training_step_outputs = []\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx) -> None:\n",
    "        self.training_step_outputs.append(outputs)\n",
    "    \n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        z_train = torch.cat([out[\"z_train\"].detach() for out in self.training_step_outputs], dim=0)\n",
    "        y_train = torch.cat([out[\"y_train\"] for out in self.training_step_outputs], dim=0)\n",
    "        z_train, y_train = z_train.cpu(), y_train.cpu()\n",
    "        self.training_step_output = []\n",
    "        \n",
    "        auc = ...\n",
    "\n",
    "        # TODO: Naucz model regresji logistycznej na parach (`z_train`, `y_train`), \n",
    "        # a następnie oblicz wartość miary AUC na zbiorze treningowym. Wykorzystaj\n",
    "        # model regresji logistycznej z biblioteki Scikit-Learn i przypisz go do\n",
    "        # pola: `self._downstream_model`.\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        self.log(\"train/auc\", auc, on_epoch=True, on_step=False)\n",
    "\n",
    "    def validation_step(self, batch: Data, batch_idx: int):\n",
    "        auc = self._compute_auc(data=batch, mask=batch.val_mask)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"val/auc\", auc, on_epoch=True, on_step=False)\n",
    "\n",
    "        return {\"auc\": auc}\n",
    "\n",
    "    def test_step(self, batch: Data, batch_idx: int):\n",
    "        auc = self._compute_auc(data=batch, mask=batch.test_mask)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"test/auc\", auc, on_epoch=True, on_step=False)\n",
    "\n",
    "        return {\"auc\": auc}\n",
    "\n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch: Data,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        z = self(batch.x, batch.edge_index)\n",
    "        y = batch.y\n",
    "\n",
    "        return z, y\n",
    "\n",
    "    def _compute_auc(self, data: Data, mask: torch.Tensor) -> float:\n",
    "        # TODO: Oblicz wartość miary AUC dla zadanego przez maskę\n",
    "        # podzbioru wierzchołków.\n",
    "        \n",
    "        auc = ...\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return auc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            params=self.parameters(),\n",
    "            lr=1e-3,\n",
    "            weight_decay=5e-4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062123cd-efe8-4fa8-8d3e-d77c34465c9a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3700355bdb8d98039df6544cb7e556de",
     "grade": true,
     "grade_id": "gae",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GraphAutoencoder(BaseUnsupervisedGNN):\n",
    "    \"\"\"Unsupervised graph autoencoder for node representations.\"\"\"\n",
    "\n",
    "    def __init__(self, gnn: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Utwórz model GAE\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # TODO: Funkcja forward powinna zwracać wektory reprezentacji `z`\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def training_step(self, batch: Data, batch_idx: int) -> dict:\n",
    "        z = ...\n",
    "        loss = ...\n",
    "\n",
    "        # TODO: Wyznacz wektory reprezentacji `z` oraz oblicz funkcję kosztu `loss`\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"train/loss\", loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"z_train\": z[batch.train_mask],\n",
    "            \"y_train\": batch.y[batch.train_mask],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data.lightning import LightningNodeData\n",
    "\n",
    "dataset = Planetoid(root=\"./data\", name=\"Cora\")\n",
    "\n",
    "datamodule = LightningNodeData(\n",
    "    data=dataset[0],\n",
    "    loader=\"full\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4ace8",
   "metadata": {},
   "source": [
    "Dla wszystkich modeli zdefiniujmy sobie zbiór wspólnych hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"hidden_dim\":  256,\n",
    "    \"emb_dim\": 128,\n",
    "}\n",
    "ACCELERATOR = \"cpu\" # change to \"cuda\" in order to use GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab47e13",
   "metadata": {},
   "source": [
    "Porównamy teraz jakość działania modeli GNNowych, wprowadzonych w poprzednim zeszycie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff69050-fd0f-4807-aeb9-8bdc595abb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv, GCNConv, SAGEConv\n",
    "\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "\n",
    "class GraphSAGEEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_dim, hidden_dim, heads=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GATConv(hidden_dim, out_dim, heads=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.trainer import get_default_trainer\n",
    "from src.utils import visualize_embeddings\n",
    "\n",
    "\n",
    "def evaluate_unsupervised_models():\n",
    "    scenarios = [\n",
    "        (\"GCN\", GCNEncoder),\n",
    "        (\"GraphSAGE\", GraphSAGEEncoder),\n",
    "        (\"GAT\", GATEncoder),\n",
    "    ]\n",
    "    \n",
    "    for model_name, gnn_cls in scenarios:\n",
    "        gnn = gnn_cls(\n",
    "            in_dim=datamodule.data.num_node_features,\n",
    "            hidden_dim=hparams[\"hidden_dim\"],\n",
    "            out_dim=hparams[\"emb_dim\"],\n",
    "        )\n",
    "    \n",
    "        model = GraphAutoencoder(gnn=gnn)\n",
    "\n",
    "        trainer = get_default_trainer(\n",
    "            num_epochs=hparams[\"num_epochs\"],\n",
    "            model_name=f\"gae_{model_name}\",\n",
    "            accelerator=ACCELERATOR,\n",
    "        )\n",
    "    \n",
    "        trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "        test_auc = trainer.test(model=model, datamodule=datamodule, verbose=False)[0][\"test/auc\"]\n",
    "        z, y = trainer.predict(model=model, datamodule=datamodule)[0]\n",
    "        z, y = z.cpu(), y.cpu()\n",
    "    \n",
    "        fig = visualize_embeddings(z=z, y=y)\n",
    "        fig.suptitle(f\"GAE {model_name} - test AUC: {test_auc * 100.:.2f} [%]\")\n",
    "    \n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "evaluate_unsupervised_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./data/logs/ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd4a19-82b3-4fa9-91db-194c1f34574f",
   "metadata": {},
   "source": [
    "## 3.2. Graph Barlow Twins\n",
    "\n",
    "Grafowe autokodery są obecnie już rzadko stosowane i zostały zastąpione przez modele należące do metod samonadzorowanych (ang. *self-supervised*). Jednym z przykładów jest model stanowiący rozszerzenie modelu Barlow Twins na dziedzinę grafów – [**Graph Barlow Twins**](https://arxiv.org/abs/2106.02466). Potok przetwarzania został przedstawiony na rysunku poniżej:\n",
    "\n",
    "![](./assets/graph-barlow-twins.png)\n",
    "\n",
    "Dla zadanego grafu wejściowego $\\mathcal{G}$ tworzone są dwa zmodyfikowane widoki $\\mathcal{G}^{(1)}, \\mathcal{G}^{(2)}$ za pomocą funkcji augmentacji (tutaj: losowe usuwanie krawędzi oraz maskowanie atrybutów wierzchołków). Następnie oba widoki są przetwarzane przez ten sam moduł kodera GNN, w wyniku czego otrzymujemy dwa zestawy (dwie macierze) reprezentacji wierzchołków $Z^{(1)}, Z^{(2)}$. Dla tych macierzy liczymy macierz korelacji wzajemnej $\\mathcal{C}$:\n",
    "\n",
    "$$\\mathcal{C}_{ij} = \\frac{\\sum_b Z^{(1)}_{b,i} Z^{(2)}_{b,j}}{\\sqrt{\\sum_b (Z^{(1)}_{b,i})^2} \\sqrt{\\sum_b (Z^{(2)}_{b,j})^2}} $$\n",
    "\n",
    "Model jest uczony za pomocą funkcji kosztu, która zbliża wartości w macierzy korelacji wzajemnej do macierzy jednostkowej:\n",
    "\n",
    "$$\\mathcal{L}_\\text{BT} = \\sum_i (1 - C_{ii})^2 + \\lambda\\sum_i\\sum_{j \\ne i} C_{ij}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d0df0-d9c1-4d0a-983a-24be1bf8e99b",
   "metadata": {},
   "source": [
    "## Zadanie 3.2 (1 pkt)\n",
    "Dokończ implementację modelu Graph Barlow Twins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f44fe-021e-45c8-b1c8-e71085b12eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class GraphAugmentor:\n",
    "    \"\"\"Masks node features (same for all nodes) and drops edges.\"\"\"\n",
    "\n",
    "    def __init__(self, p_x: float, p_e: float):\n",
    "        self._p_x = p_x\n",
    "        self._p_e = p_e\n",
    "\n",
    "    def __call__(self, data: Data):\n",
    "        x_a = mask_features(data.x, p=self._p_x)\n",
    "        x_b = mask_features(data.x, p=self._p_x)\n",
    "\n",
    "        edge_index_a = drop_edges(data.edge_index, p=self._p_e)\n",
    "        edge_index_b = drop_edges(data.edge_index, p=self._p_e)\n",
    "\n",
    "        return (x_a, edge_index_a), (x_b, edge_index_b)\n",
    "\n",
    "\n",
    "def mask_features(x: torch.Tensor, p: float) -> torch.Tensor:\n",
    "    num_features = x.size(-1)\n",
    "    device = x.device\n",
    "\n",
    "    return bernoulli_mask(size=(1, num_features), prob=p).to(device) * x\n",
    "\n",
    "\n",
    "def drop_edges(edge_index: torch.Tensor, p: float) -> torch.Tensor:\n",
    "    num_edges = edge_index.size(-1)\n",
    "    device = edge_index.device\n",
    "\n",
    "    mask = bernoulli_mask(size=num_edges, prob=p).to(device) == 1.\n",
    "\n",
    "    return edge_index[:, mask]\n",
    "\n",
    "\n",
    "def bernoulli_mask(size: int | tuple[int, ...], prob: float):\n",
    "    return torch.bernoulli((1 - prob) * torch.ones(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec54ca-0044-4eab-a92c-e2896626496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "def barlow_twins_loss(\n",
    "    z_a: torch.Tensor,\n",
    "    z_b: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    batch_size = z_a.size(0)\n",
    "    feature_dim = z_a.size(1)\n",
    "    _lambda = 1 / feature_dim\n",
    "\n",
    "    # Apply batch normalization\n",
    "    z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0) + EPS)\n",
    "    z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0) + EPS)\n",
    "\n",
    "    # Cross-correlation matrix\n",
    "    c = (z_a_norm.T @ z_b_norm) / batch_size\n",
    "\n",
    "    # Loss function\n",
    "    off_diagonal_mask = ~torch.eye(feature_dim).bool()\n",
    "    loss = (\n",
    "        (1 - c.diagonal()).pow(2).sum()\n",
    "        + _lambda * c[off_diagonal_mask].pow(2).sum()\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c3960-9f62-4b8d-a205-692eed76e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoderBN(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim, momentum=0.01)  # same as `weight_decay = 0.99`\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.bn1(self.conv1(x, edge_index)))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "\n",
    "class GraphSAGEEncoderBN(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim, momentum=0.01)  # same as `weight_decay = 0.99`\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.bn1(self.conv1(x, edge_index)))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "\n",
    "class GATEncoderBN(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_dim, hidden_dim, heads=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim, momentum=0.01)  # same as `weight_decay = 0.99`\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GATConv(hidden_dim, out_dim, heads=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.bn1(self.conv1(x, edge_index)))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c440d71-c4cd-4611-96fa-8418eabe365b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fbe38ca660dcc0b9b377ee30da02813",
     "grade": true,
     "grade_id": "gbt",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GraphBarlowTwins(BaseUnsupervisedGNN):\n",
    "    \"\"\"Self-supervised Barlow Twins model for graphs.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, p_x: float, p_e: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmentor = GraphAugmentor(p_x=p_x, p_e=p_e)\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.encoder(x, edge_index)\n",
    "\n",
    "    def training_step(self, batch: Data, batch_idx: int) -> dict:\n",
    "        # Uzupełnij potok przetwarzania modelu GBT, aby obliczyć wartość funkcji kosztu `loss`\n",
    "\n",
    "        loss = ...\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"train/loss\", loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        z = self(x=batch.x, edge_index=batch.edge_index) \n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"z_train\": z[batch.train_mask],\n",
    "            \"y_train\": batch.y[batch.train_mask],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b03e3-9d68-4e85-bc78-221902ea51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gbt_models():\n",
    "    scenarios = [\n",
    "        (\"GCN\", GCNEncoderBN),\n",
    "        (\"GraphSAGE\", GraphSAGEEncoderBN),\n",
    "        (\"GAT\", GATEncoderBN),\n",
    "    ]\n",
    "    \n",
    "    for model_name, gnn_cls in scenarios:\n",
    "        gnn = gnn_cls(\n",
    "            in_dim=datamodule.data.num_node_features,\n",
    "            hidden_dim=hparams[\"hidden_dim\"],\n",
    "            out_dim=hparams[\"emb_dim\"],\n",
    "        )\n",
    "    \n",
    "        model = GraphBarlowTwins(encoder=gnn, p_x=0.2, p_e=0.4)\n",
    "\n",
    "        trainer = get_default_trainer(\n",
    "            num_epochs=hparams[\"num_epochs\"],\n",
    "            model_name=f\"gbt_base_{model_name}\",\n",
    "            accelerator=ACCELERATOR,\n",
    "        )\n",
    "    \n",
    "        trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "        test_auc = trainer.test(model=model, datamodule=datamodule, verbose=False)[0][\"test/auc\"]\n",
    "        z, y = trainer.predict(model=model, datamodule=datamodule)[0]\n",
    "        z, y = z.cpu(), y.cpu()\n",
    "    \n",
    "        fig = visualize_embeddings(z=z, y=y)\n",
    "        fig.suptitle(f\"GBT base {model_name} - test AUC: {test_auc * 100.:.2f} [%]\")\n",
    "    \n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "evaluate_gbt_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce101c-54a9-41fc-aba9-5adc47485863",
   "metadata": {},
   "source": [
    "## Zadanie 3.3 (2 pkt)\n",
    "Zbadaj wpływ hiperparametrów maskowania atrybutów wierzchołków `p_x` oraz usuwania krawędzi `p_e` na jakość reprezentacji:\n",
    "- wybierz kilka (min. 4) wartości dla każdego hiperparametru\n",
    "- zewaluuj model dla iloczynu kartezjańskiego tych hiperparametrów\n",
    "- każdy eksperyment powtórz min. 2 razy\n",
    "- w tabelce lub na wykresie przedstaw jak dane hiperparametry wpływały na jakość reprezentacji w zadaniu docelowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615ffb0-111a-46d5-851b-5361cd14bea1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfc7e081efc87ffc0e3337bdbaf2d9c6",
     "grade": true,
     "grade_id": "gbt-hyperparams",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db8b7d-f108-4b80-94e2-e05394c95221",
   "metadata": {},
   "source": [
    "## Zadanie 3.4 (1 pkt)\n",
    "Zbadaj wpływ normalizacji `BatchNorm` na jakość reprezentacji:\n",
    "- korzystając z wyników z poprzedniego zadania, wybierz najlepszy zestaw hiperparametrów `p_x` i `p_e`\n",
    "- zewaluuj model z użyciem kodera GNNowego, który nie używa batch normalizacji \n",
    "- każdy eksperyment powtórz min. 2 razy\n",
    "- w tabelce lub na wykresie przedstaw jak użyte kodery wpływały na jakość reprezentacji w zadaniu docelowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc7062-0c16-45f3-bcfd-b6446a8d9014",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5f8aa2accb4c0364144edeca1f31cb1",
     "grade": true,
     "grade_id": "gbt-bn",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
