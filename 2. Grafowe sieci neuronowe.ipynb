{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cb87ec",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d636ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b09b0c",
   "metadata": {},
   "source": [
    "# 2. Grafowe sieci neuronowe\n",
    "\n",
    "## Motywacja\n",
    "\n",
    "### Geometric Deep Learning blueprint\n",
    "W ramach wykładu wprowadziliśmy blueprint geometrycznego uczenia głębokiego. Jednym z podstawowych punktów tego opisu było założenie o lokalnie działających funkcjach ekwiwariantnych. W połączeniu z m.in. operacjami redukcji (*pooling*) można było zbudować model, który potrafi dobrze opisywać złożone zależności w danych.\n",
    "\n",
    "W szczególności zauważyliśmy, że pojęcie lokalności w grafie jest oparte na definicji sąsiedztwa wierzchołków. Sąsiedztwem bezpośrednim $\\mathcal{N}_u$ (1-skokowym; ang. *one hop neighborhood*) wierzchołka $u$ nazywamy:\n",
    "$$\\mathcal{N}_u = \\{v \\in \\mathcal{V}: (u, v) \\in \\mathcal{E} \\lor (v, u) \\in \\mathcal{E} \\}$$\n",
    "\n",
    "Lokalnie działająca funkcja $\\phi(\\mathbf{x}_u, \\mathbf{X}_{\\mathcal{N}_u})$, która wykorzystuje atrybuty danego wierzchołka $\\mathbf{x}_u$ oraz cechy jego sąsiadów $\\mathbf{X}_{\\mathcal{N}_u}$, aby wyznaczyć wektor reprezentacji wierzchołka, pozwala na zbudowanie ekwiwariantnej względem permutacji funkcji $\\mathbf{F}$:\n",
    "\n",
    "$$\n",
    "    \\mathbf{F}(\\mathbf{X}, \\mathbf{A}) = \n",
    "        \\begin{bmatrix}\n",
    "            - \\phi(\\mathbf{x}_1, \\mathbf{X}_{\\mathcal{N}_1}) -\\\\\n",
    "            - \\phi(\\mathbf{x}_2, \\mathbf{X}_{\\mathcal{N}_2}) -\\\\\n",
    "            \\vdots \\\\\n",
    "            - \\phi(\\mathbf{x}_n, \\mathbf{X}_{\\mathcal{N}_n}) -\\\\\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Połączenie wielu takich funkcji pozwala na opisanie szerszych sąsiedztw w grafie, tzn. połącznie dwóch warstw (funkcji) pozwala opisać sąsiedztwo dwu-skokowe (2-hop), połączenie trzech funkcji – 3-hop, itd. W trakcie obecnego laboratorium zobaczymy jak można zdefiniować funkcję $\\phi$.\n",
    "\n",
    "![](./assets/local_graph_function.png)\n",
    "**Źródło**: M. M. Bronstein, J. Bruna, T. Cohen, P. Veličković, *Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges*\n",
    "\n",
    "\n",
    "### Weisfeiler-Lehman kernel\n",
    "Inną perspektywę na budowanie cech opisujących grafy poznaliśmy również na wykładzie, gdzie wprowadziliśmy algorytm kolorowania wierzchołków w celu zbadania izomorfizmu pary grafów. Na podstawie początkowo przypisanych kolorów wierzchołków, iteracyjnie każdy wierzchołek agregował multi-zbiór kolorów jego sąsiadów a następnie za pomocą funkcji haszującej $\\text{HASH}$ obliczany był nowy kolor wierzchołka:\n",
    "\n",
    "$$ c^{(k+1)}(v) = \\text{HASH}(\\{c^{(k)}(v), \\{c^{(k)}(u)\\}_{u \\in \\mathcal{N}(v)}\\})$$\n",
    "\n",
    "Powtórzenie tego kroku agregacji $K$-krotnie pozwalało opisać $K$-skokowe sąsiedztwo wierzchołków.\n",
    "\n",
    "\n",
    "**Uwaga:** Zauważmy, że przeprowadzanie $K$ iteracji jest \"równoważne\" z połączeniem $K$-warstw (funkcji) z poprzedniego przykładu.\n",
    "\n",
    "### Dodatkowe materiały\n",
    "W przypadku chęci lepszego zrozumienia oraz uzupełnienia wiedzy dot. grafowych sieci neuronowych poleca się poniższe źródła:\n",
    "- [(part 1) A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)\n",
    "- [(part 2) Understanding Convolutions on Graphs](https://distill.pub/2021/understanding-gnns/)\n",
    "- Na dłuższą lekturę: [Graph Representation Learning (William L. Hamilton, 2020)](https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61afba7-2a6b-44e0-b484-525039f43d89",
   "metadata": {},
   "source": [
    "W niniejszym zeszycie opiszemy i przeprowadzimy ewaluację 3 najpopularniejszych architektur grafowych sieci neuronowych, mianowicie:\n",
    "- Graph Convolutional Network (**GCN**)\n",
    "- Graph Sample and Aggregate (**GraphSAGE**)\n",
    "- Graph Attention Network (**GAT**)\n",
    "\n",
    "Wykorzystamy zbiór Cora i sprawdzimy jakość działania powyższych modeli w zadaniu klasyfikacji węzłów. Implementacje modeli są dostępne w bibliotece PyTorch-Geometric, natomiast do uczenia modeli wykorzystamy bibliotekę PyTorch-Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198fd85",
   "metadata": {},
   "source": [
    "## 2.1. Załadowanie zbioru\n",
    "Zbiory danych w PyTorch-Lightningu są przekazywane jako instancje obiektów `DataLoader` do metody `fit()` klasy `Trainer`. Można jednak użyć klasy `LightningDataModule` i w niej zdefiniować wszystkie data loadery wraz z wczytaniem właściwego zbioru. \n",
    "\n",
    "Zauważmy, że dotychczas rozważany zbiór danych Cora nie jest podzielony na mniejsze części (tj. mamy dokładnie jeden obiekt `Data`, który opisuje cały graf). Można wykorzystywać metody próbkowania grafów (np. `NeighborLoader`) i przez to uzyskać podział na mini-paczki, jednak zbiór Cora jest na tyle mały, że nie opłaca się go bardziej dzielić. \n",
    "\n",
    "Wykorzystamy natomiast fakt, że wczytywany obiekt `Planetoid` jest iterowalny i zwiera dokładnie jeden element. Możemy go zatem opakować w obiekt `DataListLoader` (dedykowana wersja `DataLoader`a zaimplementowana w PyTorch-Geometricu). W najnowszych wersjach PyG zaimplementowano dedykowaną klasę `LightningNodeData`, która przygotowuje grafy do przetwarzania przez modele w PyTorch-Lightningu.\n",
    "\n",
    "**Proszę zapoznać się ze znaczeniem argumentów klasy `LightningNodeData`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633c76c-3e4f-4dc4-a0fc-85d774cab7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data.lightning import LightningNodeData\n",
    "\n",
    "dataset = Planetoid(root=\"./data\", name=\"Cora\")\n",
    "\n",
    "datamodule = LightningNodeData(\n",
    "    data=dataset[0],\n",
    "    loader=\"full\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b9729",
   "metadata": {},
   "source": [
    "## 1.2. Trenowanie modeli\n",
    "Wszystkie wymienione modele grafowych sieci neuronowych zostały zaproponowane jako warstwy (z perspektywy głębokiego uczenia maszynowego), które możemy połączyć z dowolnymi innymi warstwami, aby utworzyć model (tzn. możemy wykorzystać znane aktywacje, warstwy liniowe itd.). Istotne dla nas jest jednak to, że grafowe sieci neuronowe (ang. *Graph Neural Networks - GNN*) nie są związane z jakąkolwiek funkcją kosztu i możemy je uczyć zarówno w scenariuszu nadzorowanym jak i nienadzorowanym. Temat modeli nienadzorowanych jest bardziej złożony, zatem na razie go pominiemy i wrócimy do niego w następnym zeszycie.\n",
    "\n",
    "Na chwilę obecną będziemy rozważać scenariusz nadzorowanej klasyfikacji wierzchołków. Przeanalizujmy implementację klasy `SupervisedNodeClassificationGNN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea801822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Code, display\n",
    "\n",
    "display(Code(\"src/supervised.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418a616",
   "metadata": {},
   "source": [
    "Jak już wspomnieliśmy modele grafowych sieci neuronowych mogą być stosowane w scenariuszu induktywnym. Pomimo, że przekazujemy pełną macierz atrybutów wierzchołków `data.x` oraz pełny zbiór krawędzi `data.edge_index`, funkcję kosztu oraz metrykę AUC obliczamy na wybranym pozbiorze wierzchołków. Podzbiór ten jest określany przez maskę zbioru treningowego `data.train_mask`, walidacyjnego `data.val_mask` oraz testowego `data.test_mask`. Na cele tego laboratorium możemy uznać taki scenariusz jako induktywny, jednak w rzeczywistych zastosowaniach musielibyśmy zadbać oto, aby na danym etapie uczenia (trening, walidacja, testowanie) pozostawiać tylko odpowiedni podzbiór krawędzi i wierzchołków.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.supervised import SupervisedNodeClassificationGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4ace8",
   "metadata": {},
   "source": [
    "Dla wszystkich modeli zdefiniujmy sobie zbiór wspólnych hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"hidden_dim\":  256,\n",
    "    \"emb_dim\": 128,\n",
    "}\n",
    "ACCELERATOR = \"cpu\" # change to \"cuda\" in order to use GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab47e13",
   "metadata": {},
   "source": [
    "# 2.3. Graph Convolutional Network (GCN)\n",
    "Grafowe sieci neuronowe po raz pierwszy w literaturze były proponowane już w okolicach 2008 roku, jednak dopiero rozwój i popularyzacja uczenia głębokiego pozwoliła na efektywne implementacje. Najpopularniejszym obecnie modelem grafowej sieci neuronowej jest **grafowa konwolucja** (GCN - *Graph Convolutional Network*), która została zaproponowana przez Kipfa w 2016 roku – [artykuł](https://arxiv.org/pdf/1609.02907.pdf). Praca ma już ponad 33 tysiące cytowań i wiele obecnych GNNów jest oparta na niej.\n",
    "\n",
    "Model GCN w każdej warstwie oblicza nowe cechy wierzchołków $H^{(l+1)}$ na podstawie obecnych cech $H^{(l)}$ w następujący sposób:\n",
    "\n",
    "$$H^{(l+1)} = \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)},$$\n",
    "gdzie:\n",
    "- $\\hat{A} = A + I$ to macierz sąsiedztwa grafu z dołączonymi pętlami zwrotnymi na każdym wierzchołku (krawędź z danego wierzchołka do samego siebie)\n",
    "- $\\hat{D}$ to macierz stopnii węzłów (macierz diagonalna)\n",
    "- $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ to tzw. symetryczna normalizacja macierzy sąsiedztwa\n",
    "- $W^{(l)}$ to macierz wyuczalnych parametrów\n",
    "\n",
    "Powyższa funkcja odgrywa rolę wcześniej wprowadzonej funkcji $\\phi$. Poprzez dodanie pętli na każdym wierzchołku, uśredniane są cechy zarówno sąsiadów jak i cechy danego wierzchołka. Natomiast symetryczna normalizacja pozwala uwzględnić stopień danego wierzchołka oraz stopień sąsiada.\n",
    "\n",
    "Często definicja powyższej reguły *propagacji* uwzględnia również funkcję aktywacji. W celu uniknięcia pomyłek, tutaj ją pomijamy – PyTorch-Geometric też nie stosuje funkcji aktywacji w implementacjach poszczególnych warstw.\n",
    "\n",
    "Zobaczmy jak zaimplementować grafową sieć neuronową wykorzystującą konwolucję grafową. Utworzymy sieć dwuwarstwową (uwzględniającą zatem sąsiedztwo dwu-skokowe) – zobacz klasa `GCNModel`. Następnie wykorzystamy klasę `SupervisedNodeClassificationGNN` i bibliotekę `PyTorch-Lightning`, aby nauczyć model klasyfikacji wierzchołków na zbiorze Cora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from src.trainer import get_default_trainer\n",
    "from src.utils import visualize_embeddings\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "\n",
    "    \n",
    "def evaluate_gcn():\n",
    "    gnn = GCNModel(\n",
    "        in_dim=datamodule.data.num_node_features,\n",
    "        hidden_dim=hparams[\"hidden_dim\"],\n",
    "        out_dim=hparams[\"emb_dim\"],\n",
    "    )\n",
    "    \n",
    "    model=SupervisedNodeClassificationGNN(\n",
    "        gnn=gnn, \n",
    "        emb_dim=hparams[\"emb_dim\"],\n",
    "        num_classes=len(datamodule.data.y.unique()),\n",
    "    )\n",
    "\n",
    "    trainer = get_default_trainer(\n",
    "        num_epochs=hparams[\"num_epochs\"],\n",
    "        model_name=\"supervised_GCN\",\n",
    "        accelerator = ACCELERATOR,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "    test_auc = trainer.test(model=model, datamodule=datamodule, verbose=False)[0][\"test/auc\"]\n",
    "    z, y = trainer.predict(model=model, datamodule=datamodule)[0]\n",
    "    z, y = z.cpu(), y.cpu()\n",
    "    \n",
    "    fig = visualize_embeddings(z=z, y=y)\n",
    "    fig.suptitle(f\"GCN - test AUC: {test_auc * 100.:.2f} [%]\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "evaluate_gcn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340ca4e",
   "metadata": {},
   "source": [
    "## 2.4. Graph Sample and Aggregate (GraphSAGE)\n",
    "W 2017 roku Hamilton opublikował [pracę](https://arxiv.org/pdf/1706.02216.pdf), w której rozważał induktywne uczenie grafowych sieci neuronowych oraz zaproponował sposób na osiągnięcie lepszej skalowalności metod GNNowych. Zaproponowana metoda (a właściwie rodzina metod) opiera się na idei próbkowania sąsiedztwa grafu (losowo wybrany podzbiór sąsiadów danego węzła) i następnie agregacji cech tak uzyskanej próbki sąsiadów. Zostały rozważone 3 metody agregacji: uśrednienie, LSTM oraz max pooling. Metoda była inspirowana bezpośrednio algorytmem Weisfeiler-Lehman test, a reguła propagacji jest zdefiniowana następująco (z perspektywy pojedynczego wierzchołka):\n",
    "\n",
    "$$h^{(l)}_{\\mathcal{N}(u)} = \\text{AGGREGATE}^{(l)}(\\{h^{(l)}_v, \\forall v\\in\\mathcal{N}(u) \\}) $$\n",
    "$$h^{(l+1)}_u = W^{(l)} \\cdot \\text{CONCAT}(h^{(l)}_u, h^{(l)}_{\\mathcal{N}(u)})$$\n",
    "\n",
    "Widzimy zatem, że najpierw agregujemy cechy sąsiadów za pomocą wybranej metody agregacji, a następnie konkatenujemy wektor cech danego wierzchołka ze zagregowanym wektorem sąsiadów, po czym przemnażamy taki wektor przez macierz wag.\n",
    "\n",
    "W PyTorch-Geometricu, metoda GraphSAGE (tutaj: `SAGEConv`) jest zaimplementowana delikatnie inaczej – reguła propagacji jest określona następująco (dla wariantu z uśrednieniem):\n",
    "\n",
    "$$h^{(l+1)}_u = W^{(l)}_1 \\cdot h^{(l)}_u + W^{(l)}_2 \\cdot \\text{mean}_{v\\in\\mathcal{N}(u)} h^{(l)}_v$$\n",
    "\n",
    "Cechy danego wierzchołka i zagregowanego sąsiedztwa są przekształcane przez osobne wyuczalne macierze.\n",
    "\n",
    "Analogicznie do modelu GCN przeprowadźmy ewaluację:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate_graphsage():\n",
    "    gnn = GraphSAGEModel(\n",
    "        in_dim=datamodule.data.num_node_features,\n",
    "        hidden_dim=hparams[\"hidden_dim\"],\n",
    "        out_dim=hparams[\"emb_dim\"],\n",
    "    )\n",
    "    \n",
    "    model=SupervisedNodeClassificationGNN(\n",
    "        gnn=gnn, \n",
    "        emb_dim=hparams[\"emb_dim\"],\n",
    "        num_classes=len(datamodule.data.y.unique()),\n",
    "    )\n",
    "\n",
    "    trainer = get_default_trainer(\n",
    "        num_epochs=hparams[\"num_epochs\"],\n",
    "        model_name=\"supervised_GraphSAGE\",\n",
    "        accelerator=ACCELERATOR,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "    test_auc = trainer.test(model=model, datamodule=datamodule, verbose=False)[0][\"test/auc\"]\n",
    "    z, y = trainer.predict(model=model, datamodule=datamodule)[0]\n",
    "    z, y = z.cpu(), y.cpu()\n",
    "    \n",
    "    fig = visualize_embeddings(z=z, y=y)\n",
    "    fig.suptitle(f\"GraphSAGE - test AUC: {test_auc * 100.:.2f} [%]\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "evaluate_graphsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d85b9",
   "metadata": {},
   "source": [
    "## 2.5. Graph Attention Network (GAT)\n",
    "Również w 2017 roku, Velickovic opublikował [pracę](https://arxiv.org/pdf/1710.10903.pdf), która przetłumaczyła mechanizm uwagi znany z przetwarzania języka naturalnego do dziedziny grafów. Powstała metoda nazywana Graph Attention (GAT). Reguła propagacji jest określona następująco:\n",
    "\n",
    "$$h^{(l+1)}_u = \\alpha_{u,u} W^{(l)}h^{(l)}_u + \\sum_{v \\in \\mathcal{N}(u)} \\alpha_{u,v}W^{(l)}h^{(l)}_v $$\n",
    "\n",
    "$$\\alpha_{i, j} = \\frac{\\exp(\\text{LeakyReLU}(a^T[W^{(l)}h^{(l)}_i || W^{(l)}h^{(l)}_j]))}{\\sum_{k\\in i\\cup \\mathcal{N}(i)} \\exp(\\text{LeakyReLU}(a^T[W^{(l)}h^{(l)}_i || W^{(l)}h^{(l)}_k]))},$$\n",
    "\n",
    "gdzie:\n",
    "- $W^{(l)}$ to wyuczalna macierz parametrów\n",
    "- $a$ to wyuczalne parametry mechanizmu uwagi (ang. *attention parameters*)\n",
    "- $\\alpha$ to współczynniki mechanizmu uwagi (ang. *attention coefficients*)\n",
    "\n",
    "Metoda pozwala również na wykorzystanie wielu głowic uwagi (ang. *multi-headed attention*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_dim, hidden_dim, heads=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GATConv(hidden_dim, out_dim, heads=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate_gat():\n",
    "    gnn = GATModel(\n",
    "        in_dim=datamodule.data.num_node_features,\n",
    "        hidden_dim=hparams[\"hidden_dim\"],\n",
    "        out_dim=hparams[\"emb_dim\"],\n",
    "    )\n",
    "    \n",
    "    model=SupervisedNodeClassificationGNN(\n",
    "        gnn=gnn, \n",
    "        emb_dim=hparams[\"emb_dim\"],\n",
    "        num_classes=len(datamodule.data.y.unique()),\n",
    "    )\n",
    "\n",
    "    trainer = get_default_trainer(\n",
    "        num_epochs=hparams[\"num_epochs\"],\n",
    "        model_name=\"supervised_GAT\",\n",
    "        accelerator=ACCELERATOR\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "    test_auc = trainer.test(model=model, datamodule=datamodule, verbose=False)[0][\"test/auc\"]\n",
    "    z, y = trainer.predict(model=model, datamodule=datamodule)[0]\n",
    "    z, y = z.cpu(), y.cpu()\n",
    "    \n",
    "    fig = visualize_embeddings(z=z, y=y)\n",
    "    fig.suptitle(f\"GAT - test AUC: {test_auc * 100.:.2f} [%]\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "evaluate_gat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./data/logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4c3c5",
   "metadata": {},
   "source": [
    "# Zadania\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd68d163",
   "metadata": {},
   "source": [
    "## Zadanie 2.1. Opis i ewaluacja wybranej warstwy GNN (3 pkt)\n",
    "Korzystając z listy zaimplementowanych w PyTorch-Geometricu warstw grafowych sieci neuronowych, wybierz jedną z nich, a następnie:\n",
    "\n",
    "a) Sprawdź jak w tej warstwie realizowana jest reguła propagacji – porównaj wzór z wyżej badanymi modelami oraz zastanów się jak rozumiesz zasadę działania. **Na zajęciach, powinieneś potrafić to wyjaśnić własnymi słowami.**\n",
    "\n",
    "b) Analogicznie do modeli GCN, GraphSAGE oraz GAT, zaimplementuj model dwuwarstwowy i przeprowadź jego ewaluację w zadaniu nadzorowanej klasyfikacji wierzchołków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaae24f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf8a093bc1293dfeaa8cd814ba0bdfff",
     "grade": true,
     "grade_id": "selected-gnn-implementation",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SelectedGNNModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate_selected_gnn():\n",
    "    gnn = SelectedGNNModel(\n",
    "        in_dim=datamodule.data.num_node_features,\n",
    "        hidden_dim=hparams[\"hidden_dim\"],\n",
    "        out_dim=hparams[\"emb_dim\"],\n",
    "    )\n",
    "    \n",
    "    model=SupervisedNodeClassificationGNN(\n",
    "        gnn=gnn, \n",
    "        emb_dim=hparams[\"emb_dim\"],\n",
    "        num_classes=len(datamodule.data.y.unique()),\n",
    "    )\n",
    "\n",
    "    trainer = get_default_trainer(\n",
    "        num_epochs=hparams[\"num_epochs\"],\n",
    "        model_name=\"supervised_Selected\",\n",
    "        accelerator=ACCELERATOR,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "    test_auc = trainer.test(model=model, datamodule=datamodule, verbose=False)[0][\"test/auc\"]\n",
    "    z, y = trainer.predict(model=model, datamodule=datamodule)[0]\n",
    "    z, y = z.cpu(), y.cpu()\n",
    "    \n",
    "    fig = visualize_embeddings(z=z, y=y)\n",
    "    fig.suptitle(f\"Selected - test AUC: {test_auc * 100.:.2f} [%]\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "evaluate_selected_gnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca0849",
   "metadata": {},
   "source": [
    "## Zadanie 2.2. Badanie głębokości grafowej sieci neuronowej (5 pkt)\n",
    "a) Dla trzech powyżej przedstawionych modeli GNNowych oraz wybranej w zadaniu 2.1 warstwy, zaimplementuj klasę, która utworzy grafową sieci neuronową z dowolnie określoną liczbą warstw (w poprzednich przykładach używaliśmy 2-warstowego GNNa) zadanego typu.\n",
    "\n",
    "b) Zbadaj jak głębokość grafowej sieci neuronowej wpływa na jakość uzyskiwanych reprezentacji (uwzględniając miarę AUC na zbiorze testowym). Wybierz kilka wartości dla liczby warstw i przedstaw wyniki w tabelce. Pamiętaj aby każdy eksperyment powtórzyć kilkukrotnie i podać wynik uśredniony wraz z odchyleniem standardowym. Skomentuj wyniki.\n",
    "\n",
    "c) Jak nazywa się to zaobserwowane zjawisko i wytłumacz własnymi słowami na czym polega. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af79d33",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7979edebb4b29f02274cff372d1641a",
     "grade": true,
     "grade_id": "evaluate-gnn-depth",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "class NLayerGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        layer_name: str,\n",
    "        num_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_layer(layer_name: str, in_dim: int, out_dim: int):\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate_num_layers():\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "evaluate_num_layers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
